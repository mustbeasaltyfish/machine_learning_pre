# 讲稿（Speaker Notes）

## 实际讲稿（按页逐页讲）

### 1. 机器学习实验汇报：特征提取与模型生成
大家好，今天汇报的主题是“特征提取与模型生成”。我们基于 CIFAR-10 在 MindSpore 上复现了两个方向：SimCLR 自监督表征学习和 VAE 生成模型。目标是用对比学习学到稳定表征，同时用生成模型理解潜在空间结构。接下来我会先讲模型架构，再讲优化策略，最后展示实验结果对比。

### 2. 分隔页：关键模型架构
现在进入 Section 02，我们先看两条主线模型的整体结构与分工差异，为后面的优化与结果做铺垫。

### 3. 关键模型架构：双模型并行
这一页展示两条主线：SimCLR 负责表征学习，VAE 负责潜在空间生成。SimCLR 的核心是 Encoder + Projection Head，通过对比损失学习语义特征；VAE 的核心是 Encoder + Reparameterization + Decoder，实现“编码、采样、重构”的闭环。

### 4. 适配小 Batch 的 GroupNorm
我们训练批量较小，BatchNorm 在小 batch 下统计不稳，所以改用 GroupNorm。这里把 64 通道分成 8 组进行归一化，减少对 batch 维度的依赖。这样训练更稳定，对 CIFAR-10 的小样本场景更友好。

### 5. 投影头：特征空间的非线性转换
这里是 SimCLR 的投影头。编码器输出的语义特征需要映射到对比学习的嵌入空间，两层 MLP + LayerNorm 就是为了这个目的。这个非线性投影能让对比损失更有效地优化表征质量。

### 6. 重参数化：让采样可微
这一页是 VAE 的重参数化技巧。公式是 z = mu + sigma * eps，把随机性放在 eps 上，从而让梯度能回传到 Encoder。这样就能端到端训练 VAE，而不是被采样步骤阻断。

### 7. 分隔页：优化算法与损失函数
接下来进入 Section 03，讲训练过程中的关键优化策略与损失函数实现。

### 8. SimCLR 优化：强数据增强
SimCLR 的效果很大程度上来自强增强。通过随机裁剪、翻转、亮度抖动与归一化，构造“困难正样本对”。模型因此被迫关注结构信息，而不是简单颜色差异。

### 9. 对比损失：屏蔽自相似项
这里是 NT-Xent 对比损失的关键实现。我们在 2N 相似度矩阵中用极大负值屏蔽对角线，避免模型利用自相似作弊。这样能让正负样本对的区分真正产生优化效果。

### 10. KL Annealing：逐步加权 KL 散度
VAE 训练时，如果一开始 KL 权重过大，编码器会塌缩到标准正态。我们使用 KL annealing，让 beta 从小到大逐步提升，先保证重构，再逐步约束潜在分布。

### 11. 分隔页：SimCLR 实验结果对比
进入 Section 04-1，开始展示 SimCLR 对比实验结果。

### 12. SimCLR 对比实验概览
这一页是实验概览表。我们对比了数据量、训练轮数、温度系数，并记录了最终 Loss。所有数值来自 ipynb 训练日志，保证可追溯。

### 13. 增强视角对比
这里对比不同参数下的增强样本。可以关注增强是否保持结构一致性，以及颜色和裁剪的多样性差异。这反映了正样本对“难度”在不同参数下的变化。

### 14. 相似度分布对比
这一页展示正负样本相似度分布。分布越集中、正负越分离，说明模型区分度更强。不同参数下的曲线形状能直接体现训练效果差异。

### 15. 不同参数的收敛对比
这是损失曲线对比。低温度（0.1 和 0.07）收敛最快，最终 loss 最低；数据量和训练轮数增加也能降低 loss，但幅度没有温度变化显著。

### 16. t-SNE 表征分布对比
这里是不同参数下的 t-SNE。t-SNE 展示的是表征的聚类趋势和类间分离程度。我们看到低温度条件下的分布更集中、更有区分度，但仍需结合线性评估综合判断。

### 17. Lab4-1 结果结论
Lab4-1 的结论是：温度系数对效果影响最大，其次是数据量和训练轮数。线性评估准确率 37.5% 说明表征是有效的，但仍有提升空间。下一步可以尝试更深的编码器、更大 batch 或更丰富的负样本策略。

### 18. 分隔页：VAE 实验结果与可视化
接下来进入 Section 04-2，展示 VAE 的训练与可视化结果。

### 19. VAE 实验结果概览
这一页是 VAE 结果概览：损失曲线、重构对比、生成样本以及潜在空间 t-SNE。

### 20. 损失曲线可视化
这里是 total、recon、KL 三条曲线。KL annealing 让 KL 在前期较弱、后期逐步增强，使训练更稳定，不容易塌缩。

### 21. 重构与生成结果
这页展示重构图和生成图。重构部分可以看到轮廓和结构基本保留；生成样本体现模型能从潜在空间采样出合理多样的图像。

### 22. 潜在空间 t-SNE 分布
这里是潜在向量的 t-SNE，可观察不同类别在潜在空间的聚类情况。聚类越清晰，说明潜在空间更有组织性。

### 23. Lab4-2 结果结论
Lab4-2 的结论是：KL annealing 保证了稳定训练；重构保真度较好；生成样本具备多样性。改进方向可以是更强解码器、感知损失或更丰富数据。

---

## 备用知识（扩展讲解）

### SimCLR：模型与实验知识补充
SimCLR 的核心是对比学习：同一图像的不同增强视为正样本对，不同图像视为负样本对。通过拉近正样本、推远负样本，模型学到对增强不敏感的语义表征。Projection Head 的作用是将编码器输出映射到更适合对比优化的嵌入空间。Temperature（tau）控制 softmax 的“尖锐度”，值越小，模型越严格地区分正负样本，因此通常能显著降低 loss。线性评估是对表征质量的客观验证：冻结编码器，仅训练线性分类器，能避免有监督微调带来的偏差。

### SimCLR：参数对比的意义
增加数据量提供更多负样本，能提升表征的鲁棒性；增加训练轮数让模型有更多机会收敛，但后期收益递减；温度系数的调整直接影响对比损失的梯度分布，通常是最敏感的参数。实验中看到 tau 从 0.5 降到 0.07 带来显著改善，符合 SimCLR 原论文推荐。

### VAE：模型与实验知识补充
VAE 将自编码器与概率模型结合，编码器输出潜在分布的参数（mu、logvar），解码器从潜在变量 z 重构输入。重参数化技巧把随机性转移到 eps 上，使采样可微，避免梯度断流。损失由重构损失与 KL 散度组成，KL 用于让潜在分布接近标准正态。KL annealing 逐步提高 beta，避免训练初期 KL 过强导致“塌缩”，让模型先学会重构再学习分布约束。

### 结果解读与改进方向
Loss 曲线重点看趋势是否稳定，避免只关注终值。t-SNE 仅提供可视化线索，不能作为严格量化指标。重构与生成图优先关注结构、轮廓和多样性，而非像素级完美。SimCLR 可通过更深编码器、更大 batch、memory bank 或更强增强策略提升效果；VAE 可通过感知损失、对抗训练或更强解码器改善生成质量。
